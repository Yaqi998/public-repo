In recent years, the rapid advancement of artificial intelligence has been marked by the emergence of Transformers, such as OpenAI's GPT series and Google's Bard. These models are powerful tools for natural language understanding and generation, However, despite their impressive capabilities, Transformers face critical challenges, particularly when tasked with staying current, managing large-scale information, These limitations have prompted the integration of Retrieval-Augmented Generation (RAG) techniques, which combine Transformers with external knowledge retrieval systems to enhance their reliability and applicability. 

To efficiently retrieve information from vast internal data sources, Vespa, a vectorized database, is utilized. In Vespa, data is encoded as "contracts," which are represented as high-dimensional vectors in an embedding space. These vectors are organized based on their relative distances, allowing the database to group similar contracts into clusters. These clusters are referred to as "catalogs." The clustering in the space can cut the latency of the response time. 

When a query is processed, it is transformed into a vector in the same embedding space. The system identifies the catalog with the closest proximity to this query vector, ensuring highly relevant results. 

In the context of the GBMBot, a contracts is a high-level definition of the API within the firm. similar APIs are grouped together to form a catalog, The query given by users are transformed into a vector, which is passed on to RAG to compare with the contracts (API description) in the vector space to select the most suitable API. The information in the API description is used to augment the information in the original query and feed back to the LLM, which then extracts the information from the query and generate input to the selected API. The answer from that API is then given back to the user.